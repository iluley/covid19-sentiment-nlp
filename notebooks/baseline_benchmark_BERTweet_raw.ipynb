{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "87d39bd7",
      "metadata": {
        "id": "87d39bd7"
      },
      "source": [
        "# Natural Languange Processing (NLP)- Advanced Topics in DL\n",
        "\n",
        "Group Z:</br>\n",
        "    - Iliya Morgunov - 206361412</br>\n",
        "    - Eadan Schechter - 209793553"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83aec080",
      "metadata": {
        "id": "83aec080"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9be550a7",
      "metadata": {
        "id": "9be550a7"
      },
      "outputs": [],
      "source": [
        "#pip install -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d97ffcf5",
      "metadata": {
        "id": "d97ffcf5"
      },
      "outputs": [],
      "source": [
        "#pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zlikYuduUDV4",
      "metadata": {
        "id": "zlikYuduUDV4"
      },
      "outputs": [],
      "source": [
        "#pip install optuna wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cedf723",
      "metadata": {
        "id": "9cedf723"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
        "\n",
        "import time\n",
        "import os\n",
        "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
        "import shutil\n",
        "import re\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, EarlyStoppingCallback, Trainer\n",
        "from datasets import Dataset as HFDataset        # Hugging Face\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset as TorchDataset, DataLoader  # PyTorch\n",
        "import torch.nn.functional as F\n",
        "from torch.quantization import quantize_dynamic\n",
        "from torch.nn.utils import prune\n",
        "\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "import optuna\n",
        "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
        "import wandb\n",
        "wandb.login(key=\"a4366556f2db644bb48872fb5da34a12fcdb200d\")\n",
        "\n",
        "\n",
        "import warnings\n",
        "from transformers.utils import logging as hf_logging\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "hf_logging.set_verbosity_error()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7327245",
      "metadata": {
        "id": "b7327245"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(\"df_train_final.csv\")\n",
        "df_test = pd.read_csv(\"df_test_final.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "788fef5e",
      "metadata": {
        "id": "788fef5e"
      },
      "source": [
        "# Part 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4c8cfbd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4c8cfbd",
        "outputId": "9925e8f9-9e97-4b10-a5c2-7c428db647eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "12.4\n",
            "90300\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.version.cuda)\n",
        "print(torch.backends.cudnn.version())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b924d18",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b924d18",
        "outputId": "4a9d2634-49cd-4b96-adc4-d562b7449a12"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a11c91a0",
      "metadata": {
        "id": "a11c91a0"
      },
      "source": [
        "## Fine-tuning BERTweet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a831d63",
      "metadata": {
        "id": "8a831d63"
      },
      "source": [
        "BERTweet is a RoBERTa-based language model specifically pre-trained on over 850 million English tweets to capture the linguistic nuances and informal conventions of social media text. Academic research has shown that BERTweet achieves state-of-the-art results for tweet sentiment analysis. For example, Nguyen et al. (2020) demonstrated that BERTweet substantially outperformed general-domain models like RoBERTa and XLM-R when classifying tweet sentiment, improving the previous state-of-the-art by approximately 5% absolute F1 on the SemEval-2017 Twitter sentiment benchmark. This improvement translates to a significantly higher accuracy in classifying tweets as positive, negative, or neutral, compared to earlier transformer-based approaches. The model’s strong performance is attributed to its large-scale, domain-specific pre-training, which allows it to recognize Twitter slang, emojis, hashtags, and informal syntax much better than generic BERT variants.\n",
        "\n",
        "The BERTweet pre-training corpus is highly relevant to our own project. It consists of an 80GB collection of uncompressed text containing 850 million tweets (about 16 billion word tokens), gathered from two main sources. The first and largest corpus (845M tweets) was collected from the Twitter Stream Archive, spanning from January 2012 to August 2019, and includes only English-language tweets. These tweets were pre-processed with the TweetTokenizer from the NLTK toolkit, and emojis were converted into text. User mentions and URLs were normalized into special tokens (@USER, HTTPURL), and only tweets containing between 10 and 64 tokens were retained, with retweets excluded. Notably, the second component of the pre-training data includes 5 million English tweets related to the COVID-19 pandemic, collected between January and March 2020—an interval that partially overlaps with the timeline of the data in our own analysis. This overlap suggests that BERTweet is well-equipped to handle the vocabulary, topics, and sentiment expressions specific to the COVID-19 period.\n",
        "\n",
        "BERTweet is publicly available and can be easily downloaded (e.g., from the Hugging Face Hub as vinai/bertweet-base) and fine-tuned in PyTorch for sentiment classification tasks. In summary, BERTweet represents a best-in-class choice for Twitter sentiment analysis, as it combines the robust RoBERTa architecture with domain knowledge of tweets—including some from the same time window as our dataset—enabling more accurate and contextually aware sentiment modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d764186",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d764186",
        "outputId": "d2fe9407-793e-4b45-cdba-3c299ce2925b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(130, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=5, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the BERTweet model and tokenizer and inspect\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\n",
        "try:\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\"vinai/bertweet-base\", num_labels=5).to(device) # We have 5 sentiments\n",
        "except:\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\"vinai/bertweet-base\", num_labels=5, use_safetensors=True).to(device) # We have 5 sentiments\n",
        "\n",
        "#model = AutoModelForSequenceClassification.from_pretrained(\"vinai/bertweet-base\", num_labels=5, use_safetensors=True).to(device) # We have 5 sentiments\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c843abb7",
      "metadata": {
        "id": "c843abb7"
      },
      "source": [
        "### PyTorch Fine-Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16488016",
      "metadata": {
        "id": "16488016"
      },
      "source": [
        "The TweetDataset class inherits from PyTorch’s Dataset class, enabling us to efficiently handle and preprocess tweet data for model training.\n",
        "\n",
        "This custom dataset class is especially useful for working with data formats *not directly supported by PyTorch's built-in datasets, and text data that requires tokenization and transformation before being fed into a neural network.\n",
        "\n",
        "The three essential methods are:\n",
        "\n",
        "__init__: Initializes the dataset with the raw tweet texts, corresponding sentiment labels, tokenizer, and any additional settings.\n",
        "\n",
        "__len__: Returns the total number of tweet samples in the dataset, which allows PyTorch DataLoader to know how many batches to create.\n",
        "\n",
        "__getitem__: Retrieves a single tweet (data sample) and its label by index, applies the tokenizer to convert the tweet into input features (input_ids, attention_mask), and returns the tensors ready for model input.\n",
        "\n",
        "This structure allows for flexible and efficient batching, shuffling, and preprocessing of tweets during model training and evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b46ed7fc",
      "metadata": {
        "id": "b46ed7fc"
      },
      "source": [
        "We now implement the TweetDataset class and prepare our data for PyTorch training. The sequence length was selected based on our EDA (see previous section), and all data and labels are processed for efficient batching.\n",
        "\n",
        "For model training, we use a standard fine-tuning pipeline with early stopping, hyperparameter tuning via Optuna, and experiment tracking with Weights & Biases (W&B), following best practices introduced in course Exercise 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9aa47364",
      "metadata": {
        "id": "9aa47364"
      },
      "outputs": [],
      "source": [
        "class TweetDataset(TorchDataset): # Dataset Class\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for tweet sentiment analysis using a DataFrame.\n",
        "    Processes each row to create tokenized input for transformer models\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, text_col=\"OriginalTweet\", label_col=\"label\", max_len=80):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataframe (pd.DataFrame): DataFrame containing tweets and labels.\n",
        "            tokenizer: Hugging Face tokenizer instance.\n",
        "            text_col (str): Column name containing tweet text.\n",
        "            label_col (str): Column name containing sentiment label (as integer or already encoded).\n",
        "            max_len (int): Maximum sequence length for model input.\n",
        "        \"\"\"\n",
        "\n",
        "        self.texts = dataframe[text_col].astype(str).tolist()\n",
        "        self.labels = dataframe[label_col].tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len # Ensures all sequences have the same length (Maximum tweet length allowed)\n",
        "\n",
        "    def __len__(self):\n",
        "        # Returns total number of tweets\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "         # Retrieves and processes a single tweet at the given index.\n",
        "\n",
        "        text = self.texts[idx]     # Raw tweet text\n",
        "        label = self.labels[idx]        # Corresponding sentiment label\n",
        "\n",
        "        # Tokenize the tweet with padding/truncation to max_len using the provided tokenizer\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,    # Pad/truncate to this length (Ensures uniform input size)\n",
        "            padding='max_length',       # Add padding to reach the maximum length (pad shorter tweets to max_len)\n",
        "            truncation=True,            # Truncate if longer than max_len\n",
        "            return_attention_mask=True, # Generate attention mask for BERT\n",
        "            return_tensors='pt'         # Output as PyTorch tensors\n",
        "        )\n",
        "\n",
        "        #  Return a dictionary containing tokenized inputs, attention masks, and the label\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),           # Tensor of size [max_len]\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(), # Tensor of size [max_len]\n",
        "            'labels': torch.tensor(label, dtype=torch.long)         # Scalar tensor\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f73f0873",
      "metadata": {
        "id": "f73f0873"
      },
      "source": [
        "Apply Label Encoding to the data set sentiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02c41eef",
      "metadata": {
        "id": "02c41eef"
      },
      "outputs": [],
      "source": [
        "# Map Sentiment to 0-4 integers (sentiment columns are now integer incoded)\n",
        "sentiment_classes = ['Negative', 'Neutral', 'Positive', 'Extremely Negative', 'Extremely Positive']\n",
        "label2id = {label: idx for idx, label in enumerate(sentiment_classes)}\n",
        "df_test['label'] = df_test['Sentiment'].map(label2id) # Full test set\n",
        "df_train['label'] = df_train['Sentiment'].map(label2id) # Full train set"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b250923",
      "metadata": {
        "id": "0b250923"
      },
      "source": [
        "#### Early stopping function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "931051ef",
      "metadata": {
        "id": "931051ef"
      },
      "outputs": [],
      "source": [
        "def early_stop_check(patience, best_val_f1, best_val_f1_epoch, current_val_f1, current_epoch):\n",
        "    early_stop_flag = False # Initialize flag to be False (no need to early stop\n",
        "    if current_val_f1 > best_val_f1:\n",
        "        # If we improved the F1, update the parameteres holding the best val f1 details\n",
        "        best_val_f1 = current_val_f1\n",
        "        best_val_f1_epoch = current_epoch\n",
        "    else:\n",
        "        # No improvement, check for patience: check if there has been more than the acceptable number of epochs where val f1 hasn't improved\n",
        "        if current_epoch - best_val_f1_epoch > patience:\n",
        "            early_stop_flag = True # Change flag\n",
        "    return best_val_f1, best_val_f1_epoch, early_stop_flag"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adca1666",
      "metadata": {
        "id": "adca1666"
      },
      "source": [
        "#### Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7eacf8c7",
      "metadata": {
        "id": "7eacf8c7"
      },
      "outputs": [],
      "source": [
        "def train_model_with_hyperparams(model, model_name, train_loader, val_loader, optimizer, criterion, epochs, patience, trial):\n",
        "    best_val_f1  = 0.0 # Keep track of best f1\n",
        "    best_val_f1_epoch  = 0\n",
        "    early_stop_flag = False\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train() # Enable training mode\n",
        "        train_loss = 0.0\n",
        "        total_train_samples = 0\n",
        "        correct_train_predictions = 0\n",
        "\n",
        "        for batch in train_loader: #Iterates over the train_loader, which is a DataLoader object containing batches of training data.\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            optimizer.zero_grad() # Reset gradients\n",
        "            outputs = model(input_ids, attention_mask=attention_mask) # Forward pass\n",
        "            logits = outputs.logits # save the logits (the raw output of the model)\n",
        "            loss = criterion(logits, labels) # Calculate loss\n",
        "\n",
        "            loss.backward() # Backward pass\n",
        "            optimizer.step() # Update weights using the optimizer\n",
        "\n",
        "            # Accumulate training loss and predictions\n",
        "            train_loss += loss.item() * input_ids.size(0)\n",
        "            total_train_samples += input_ids.size(0)\n",
        "            correct_train_predictions += (logits.argmax(dim=1) == labels).sum().item()\n",
        "\n",
        "        train_loss /= total_train_samples\n",
        "        train_accuracy = correct_train_predictions / total_train_samples\n",
        "\n",
        "        ###  Validation loop  ###\n",
        "        model.eval() # Enable evaluation mode\n",
        "        val_loss = 0.0\n",
        "        total_val_samples = 0\n",
        "        correct_val_predictions = 0\n",
        "\n",
        "        all_val_labels = []\n",
        "        all_val_preds = []\n",
        "\n",
        "        with torch.no_grad(): # Disable gradient computation\n",
        "            for batch in val_loader: # iterate on the val_loader's batches\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                logits = outputs.logits\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "                val_loss += loss.item() * input_ids.size(0)\n",
        "                total_val_samples += input_ids.size(0)\n",
        "                correct_val_predictions += (logits.argmax(dim=1) == labels).sum().item()\n",
        "\n",
        "                all_val_labels.extend(labels.cpu().numpy())\n",
        "                all_val_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "        # calculate metrics\n",
        "        val_loss /= total_val_samples\n",
        "        val_accuracy = correct_val_predictions / total_val_samples\n",
        "        val_precision = precision_score(all_val_labels, all_val_preds, average='macro')\n",
        "        val_recall = recall_score(all_val_labels, all_val_preds, average='macro')\n",
        "        val_f1 = f1_score(all_val_labels, all_val_preds, average='macro')\n",
        "\n",
        "        if patience is not None:\n",
        "            # Check for early stopping\n",
        "            best_val_f1, best_val_f1_epoch, early_stop_flag = early_stop_check(patience, best_val_f1, best_val_f1_epoch, val_f1, epoch)\n",
        "\n",
        "        # Save the best model under the best_model_state parameter\n",
        "        if val_f1 == best_val_f1:\n",
        "            best_model_state = model.state_dict()\n",
        "\n",
        "        # Log metrics to Weights & Biases\n",
        "        if wandb.run is not None:\n",
        "            wandb.log({\n",
        "                \"Epoch\": epoch,\n",
        "                \"Train Loss\": train_loss,\n",
        "                \"Train Accuracy\": train_accuracy,\n",
        "                \"Validation Loss\": val_loss,\n",
        "                \"Validation Accuracy\": val_accuracy,\n",
        "                \"Validation Precision\": val_precision,\n",
        "                \"Validation Recall\": val_recall,\n",
        "                \"Validation F1\": val_f1})\n",
        "\n",
        "        if early_stop_flag:  # Checks whether the early stopping condition has been met, as indicated by the early_stop_flag\n",
        "            break # Exits the training loop immediately if the early stopping condition is satisfied\n",
        "\n",
        "    if best_model_state is not None:\n",
        "    # Save the best model if tracked (e.g., via early stopping or best F1)\n",
        "        if trial is not None:\n",
        "            torch.save(best_model_state, f\"best_model_trial_{trial.number}.pt\")\n",
        "        else:\n",
        "            torch.save(best_model_state, f\"{model_name.replace('/', '-')}_fine_tuned_pytorch_model.pt\")\n",
        "    else:\n",
        "        # No best state was tracked — save the final model as-is\n",
        "        torch.save(model.state_dict(), f\"{model_name.replace('/', '-')}_fine_tuned_pytorch_model.pt\")\n",
        "\n",
        "    return val_f1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e958a18",
      "metadata": {
        "id": "5e958a18"
      },
      "source": [
        "#### Optuna Function & Study"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec17709c",
      "metadata": {
        "id": "ec17709c"
      },
      "outputs": [],
      "source": [
        "# Objective Function for Optuna\n",
        "def objective(trial, train_df, tokenizer, model_name, text_col, label_col, device):\n",
        "\n",
        "    \"\"\"\n",
        "    Generic Optuna objective function for HuggingFace transformer-based models (e.g., BERTweet, DeBERTa).\n",
        "    \"\"\"\n",
        "\n",
        "    # Hyperparameter suggestions\n",
        "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n",
        "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-4)\n",
        "    patience = trial.suggest_int(\"patience\", 5, 7)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64])\n",
        "    epochs = trial.suggest_int(\"epochs\", 10, 25, step=5)\n",
        "    num_layers = trial.suggest_int(\"num_layers\", 1, 1)\n",
        "\n",
        "    # For cross-validation\n",
        "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "    f1_scores = []\n",
        "\n",
        "    # Extract features and labels as lists (so indices match)\n",
        "    X = train_df[text_col].tolist()\n",
        "    y = train_df[label_col].tolist()\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "        X_train = [X[i] for i in train_idx]\n",
        "        y_train = [y[i] for i in train_idx]\n",
        "        X_val = [X[i] for i in val_idx]\n",
        "        y_val = [y[i] for i in val_idx]\n",
        "\n",
        "        train_fold_df = pd.DataFrame({text_col: X_train, label_col: y_train})\n",
        "        val_fold_df = pd.DataFrame({text_col: X_val, label_col: y_val})\n",
        "\n",
        "        train_dataset = TweetDataset(train_fold_df, tokenizer, text_col=text_col, label_col=label_col, max_len=80) # Create the TweetDataset object\n",
        "        val_dataset = TweetDataset(val_fold_df, tokenizer, text_col=text_col, label_col=label_col, max_len=80) # Create the TweetDataset object\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # Insert into a DataLoader\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False) # Insert into a DataLoader\n",
        "\n",
        "        # Initialize a model (generic) for each fold\n",
        "        try:\n",
        "            model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5).to(device)\n",
        "        except:\n",
        "            model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5, use_safetensors=True).to(device)\n",
        "\n",
        "        #model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5, use_safetensors=True).to(device)\n",
        "\n",
        "        # Freezing logic for DeBERTa\n",
        "        if hasattr(model, \"deberta\") and hasattr(model.deberta, \"encoder\"):\n",
        "            # Freeze all DeBERTa encoder parameters\n",
        "            for param in model.deberta.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "            # Unfreeze the last `num_layers` DeBERTa encoder layers\n",
        "            encoder_layers = model.deberta.encoder.layer\n",
        "            for layer in encoder_layers[-num_layers:]:\n",
        "                for param in layer.parameters():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "        # Freezing logic for RoBERTa type models (e.g BERTweet)\n",
        "        elif hasattr(model, \"roberta\") and hasattr(model.roberta, \"encoder\"):\n",
        "            for param in model.roberta.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "            encoder_layers = model.roberta.encoder.layer\n",
        "            for layer in encoder_layers[-num_layers:]:\n",
        "                for param in layer.parameters():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "        # Always unfreeze classifier (for any model applied)\n",
        "        for param in model.classifier.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # Define optimizer and loss function\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "        # Initialize Weights & Biases logging - the values in the config are the properties of each trial.\n",
        "        wandb.init(project=f\"{model_name.replace('/', '-')} Sentiment Analysis Fine-Tuning Full Code - {datetime.now().date().strftime('%Y%m%d')}\",\n",
        "            config={\n",
        "            \"learning_rate\": learning_rate,\n",
        "            \"weight_decay\": weight_decay,\n",
        "            \"patience\": patience,\n",
        "            \"batch_size\": batch_size,\n",
        "            \"num_layers\": num_layers,\n",
        "            \"architecture\": model_name,\n",
        "            \"dataset\": \"Corona_NLP\"\n",
        "            },\n",
        "            name=f\"trial_{trial.number}_fold_{fold}\", # The name that will be saved in the W&B platform\n",
        "            reinit=True)\n",
        "\n",
        "        # Train the model for this fold and get the best validation f1\n",
        "        best_val_f1 = train_model_with_hyperparams(model, model_name, train_loader, val_loader, optimizer, criterion, epochs = epochs, patience=patience, trial=trial)\n",
        "\n",
        "\n",
        "        # Append fold validation accuracy\n",
        "        f1_scores.append(best_val_f1)\n",
        "\n",
        "        # Finish the Weights & Biases run\n",
        "        wandb.finish()\n",
        "\n",
        "    mean_f1 = np.mean(f1_scores)\n",
        "\n",
        "    # Clean up CUDA + wandb\n",
        "    import gc\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    if wandb.run is not None:\n",
        "        wandb.finish()\n",
        "\n",
        "    return mean_f1 # Return best mean validation f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63a9f8aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "63a9f8aa",
        "outputId": "29b4ff41-bea9-445d-c758-6d98837f1194"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Optuna Trials:   0%|          | 0/3 [00:37<?, ?it/s]\n",
            "\n",
            "Optuna Trials:  33%|███▎      | 1/3 [56:34<1:53:09, 3394.57s/it]\u001b[A\n",
            "Optuna Trials:  67%|██████▋   | 2/3 [1:34:30<45:36, 2736.71s/it]\u001b[A\n",
            "Optuna Trials: 100%|██████████| 3/3 [2:06:09<00:00, 2523.30s/it]\n"
          ]
        }
      ],
      "source": [
        "n_trials = 3\n",
        "pbar = tqdm(total=n_trials, desc=\"Optuna Trials\")\n",
        "\n",
        "def tqdm_callback(study, trial):\n",
        "    pbar.update(1)\n",
        "\n",
        "# Optimize - Run Optuna study\n",
        "bertweet_study = optuna.create_study(direction=\"maximize\") # Specifies that the goal of the optimization is to maximize the objective function (optimize mean f1)\n",
        "bertweet_study.optimize(lambda trial: objective(trial, df_train, tokenizer, \"vinai/bertweet-base\", \"OriginalTweet\", \"label\", device), n_trials=n_trials, callbacks=[tqdm_callback])\n",
        "\n",
        "pbar.close()\n",
        "\n",
        "# Get best trial info\n",
        "best_trial = bertweet_study.best_trial\n",
        "bertweet_best_params = bertweet_study.best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb820bf6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb820bf6",
        "outputId": "c50c46c6-ace0-4e49-e1c5-46273cd955ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best trial:\n",
            "  Validation F1-score: 0.6386208620181888\n",
            "  Hyperparameters: \n",
            "    learning_rate: 0.000576610266987812\n",
            "    weight_decay: 4.3717724210425024e-06\n",
            "    patience: 7\n",
            "    batch_size: 32\n",
            "    epochs: 10\n",
            "    num_layers: 1\n"
          ]
        }
      ],
      "source": [
        "# Print the best hyperparameters\n",
        "print(\"Best trial:\")\n",
        "print(f\"  Validation F1-score: {best_trial.value}\")\n",
        "print(\"  Hyperparameters: \")\n",
        "for key, value in best_trial.params.items():\n",
        "    print(f\"    {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc61d8d0",
      "metadata": {
        "id": "cc61d8d0"
      },
      "source": [
        "#### Train again and test\n",
        "\n",
        "Instead of saving the best model from a single fold during cross-validation, we need to retrain the model from scratch on the entire training set (excluding the test set), using the best hyperparameters selected by Optuna. This is applied in order to:\n",
        "Maximizing Training Data: Retraining on the full dataset allowed the model to benefit from more examples than in any single fold, potentially improving generalization\n",
        "Avoiding Fold Bias: Since no single fold sees the full training pool during cross-validation, retraining ensures the final model is not biased toward a specific validation split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c6fb765",
      "metadata": {
        "id": "4c6fb765"
      },
      "outputs": [],
      "source": [
        "def retrain_transformer_full(train_df, test_df, best_params, tokenizer, model_name, label_col, text_col, device=None):\n",
        "\n",
        "    \"\"\"\n",
        "    Retrain the model on the full train_df using the best found hyperparameters form the Optuna study\n",
        "    \"\"\"\n",
        "\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Prepare dataset and dataloader for full train\n",
        "    train_dataset = TweetDataset(train_df, tokenizer, text_col=text_col, label_col=label_col, max_len=80)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
        "\n",
        "\n",
        "    test_dataset = TweetDataset(test_df, tokenizer, text_col=text_col, label_col=label_col, max_len=80)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=best_params['batch_size'], shuffle=False)\n",
        "\n",
        "    # Initialize a fresh model for final training\n",
        "    try:\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5).to(device)\n",
        "    except:\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5, use_safetensors=True).to(device)\n",
        "\n",
        "    #model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5, use_safetensors=True).to(device)\n",
        "\n",
        "    # Freeze & unfreeze layers according to best_params\n",
        "    num_layers = best_params['num_layers']\n",
        "\n",
        "    # Freezing logic for DeBERTa\n",
        "    if hasattr(model, \"deberta\") and hasattr(model.deberta, \"encoder\"):\n",
        "        # Freeze all DeBERTa encoder parameters\n",
        "        for param in model.deberta.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Unfreeze the last `num_layers` DeBERTa encoder layers\n",
        "        encoder_layers = model.deberta.encoder.layer\n",
        "        for layer in encoder_layers[-num_layers:]:\n",
        "            for param in layer.parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "    # Freezing logic for RoBERTa type models (e.g BERTweet)\n",
        "    elif hasattr(model, \"roberta\") and hasattr(model.roberta, \"encoder\"):\n",
        "        for param in model.roberta.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Unfreeze the last `num_layers` of the encoder layers\n",
        "        encoder_layers = model.roberta.encoder.layer\n",
        "        for layer in encoder_layers[-num_layers:]:\n",
        "            for param in layer.parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "    # Always unfreeze classifier (for any model applied)\n",
        "    for param in model.classifier.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Start W&B logging for the final training\n",
        "    run_name = f\"Final_Train_{model_name.replace('/', '-')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "    wandb.init(\n",
        "        project=f\"Final_{model_name.replace('/', '-')}_Retrain_{datetime.now().date().strftime('%Y%m%d')}\",\n",
        "        config=best_params,\n",
        "        name=run_name,\n",
        "        reinit=True,\n",
        "    )\n",
        "\n",
        "    # Retrain model on train_loader and evaluate on test_loader\n",
        "    train_model_with_hyperparams(\n",
        "        model,\n",
        "        model_name=model_name,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=test_loader,\n",
        "        optimizer=optimizer,\n",
        "        criterion=criterion,\n",
        "        epochs=best_params['epochs'],\n",
        "        patience=None,  # No early stopping\n",
        "        trial=None      # Not an Optuna trial anymore\n",
        "    )\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac115074",
      "metadata": {
        "id": "ac115074"
      },
      "outputs": [],
      "source": [
        "# BERTweet tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\n",
        "\n",
        "final_bertweet_model = retrain_transformer_full(\n",
        "    train_df=df_train,\n",
        "    test_df=df_test,\n",
        "    best_params=bertweet_best_params,\n",
        "    tokenizer=tokenizer,\n",
        "    model_name='vinai/bertweet-base',\n",
        "    label_col='label',\n",
        "    text_col='OriginalTweet',\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3f4f35e",
      "metadata": {
        "id": "c3f4f35e"
      },
      "source": [
        "#### Evaluation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "188b6e5b",
      "metadata": {
        "id": "188b6e5b"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader, criterion, device=None):\n",
        "    # Infer device from model if not provided\n",
        "    if device is None:\n",
        "        device = next(model.parameters()).device\n",
        "\n",
        "    model.eval()\n",
        "    all_labels, all_preds = [], []\n",
        "    total_loss, total_samples = 0.0, 0\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            # Move batch to the SAME device as the model\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            # label key can be 'labels' (PyTorch dataset) or 'label' (HF dataset); here it's 'labels'\n",
        "            labels = (batch.get('labels') if 'labels' in batch else batch['label']).to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            total_loss += loss.item() * input_ids.size(0)\n",
        "            total_samples += input_ids.size(0)\n",
        "\n",
        "            preds = logits.argmax(dim=1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    end_time = time.time()\n",
        "    inference_time = end_time - start_time\n",
        "    inference_time_per_sample = inference_time / max(total_samples, 1)\n",
        "\n",
        "    avg_loss = total_loss / max(total_samples, 1)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    macro_precision = precision_score(all_labels, all_preds, average='macro')\n",
        "    macro_recall = recall_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "    # Model size (MB)\n",
        "    torch.save(model.state_dict(), \"tmp_model_eval.pt\")\n",
        "    model_size_mb = os.path.getsize(\"tmp_model_eval.pt\") / 1e6\n",
        "    os.remove(\"tmp_model_eval.pt\")\n",
        "\n",
        "    # Parameter count\n",
        "    param_count = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "    return {\n",
        "        \"Loss\": avg_loss,\n",
        "        \"F1 Score (macro)\": macro_f1,\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"Precision (macro)\": macro_precision,\n",
        "        \"Recall (macro)\": macro_recall,\n",
        "        \"Inference Time (sec)\": inference_time,\n",
        "        \"Inference Time (sec/sample)\": inference_time_per_sample,\n",
        "        \"Model Size (Mb)\": model_size_mb,\n",
        "        \"Parameter Count\": param_count\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b50c5fe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b50c5fe",
        "outputId": "0921fd04-e3f4-43de-c3d1-55c2e5c962d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 0.9788\n",
            "F1 Score (macro): 0.6219\n",
            "Accuracy: 0.6058\n",
            "Precision (macro): 0.6258\n",
            "Recall (macro): 0.6243\n",
            "Inference Time (sec): 5.3786\n",
            "Inference Time (sec/sample): 0.0014\n",
            "Model Size (Mb): 539.6995\n",
            "Parameter Count: 134903813\n"
          ]
        }
      ],
      "source": [
        "# Re-create test_dataset and test_loader outside\n",
        "test_dataset = TweetDataset(df_test, tokenizer, text_col=\"OriginalTweet\", label_col=\"label\", max_len=80)\n",
        "test_loader = DataLoader(test_dataset, batch_size=bertweet_best_params['batch_size'], shuffle=False)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "metrics = evaluate_model(final_bertweet_model, test_loader, criterion, device)\n",
        "for k, v in metrics.items():\n",
        "    print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "DL_ex_1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
